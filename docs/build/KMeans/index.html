<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>K-Means · AdvChemStat.jl</title><meta name="title" content="K-Means · AdvChemStat.jl"/><meta property="og:title" content="K-Means · AdvChemStat.jl"/><meta property="twitter:title" content="K-Means · AdvChemStat.jl"/><meta name="description" content="Documentation for AdvChemStat.jl."/><meta property="og:description" content="Documentation for AdvChemStat.jl."/><meta property="twitter:description" content="Documentation for AdvChemStat.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="AdvChemStat.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">AdvChemStat.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../svd/">SVD</a></li><li><a class="tocitem" href="../HCA/">HCA</a></li><li class="is-active"><a class="tocitem" href>K-Means</a><ul class="internal"><li><a class="tocitem" href="#Introduction"><span>Introduction</span></a></li><li><a class="tocitem" href="#How?"><span>How?</span></a></li><li><a class="tocitem" href="#Practical-Example"><span>Practical Example</span></a></li><li><a class="tocitem" href="#Applications"><span>Applications</span></a></li><li><a class="tocitem" href="#Additional-Example"><span>Additional Example</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>K-Means</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>K-Means</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/EMCMS/AdvChemStat.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/EMCMS/AdvChemStat.jl/blob/main/docs/src/KMeans.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="K-means-Clustering"><a class="docs-heading-anchor" href="#K-means-Clustering">K-means Clustering</a><a id="K-means-Clustering-1"></a><a class="docs-heading-anchor-permalink" href="#K-means-Clustering" title="Permalink"></a></h1><h2 id="Introduction"><a class="docs-heading-anchor" href="#Introduction">Introduction</a><a id="Introduction-1"></a><a class="docs-heading-anchor-permalink" href="#Introduction" title="Permalink"></a></h2><p>The <a href="https://en.wikipedia.org/wiki/K-means_clustering"><strong>k-means clustering</strong></a> is an unsupervised clustering algorithm that uses the distance metrics to create the user defined number of clusters. The <strong>k-means</strong> can be used for both continuous and discreet data, given that there are at least two variables provided. The number of clusters to be formed are typically provided by the user and is typically based on the <em>prior</em> knowledge. The center of each cluster in <strong>k-means</strong> is called <em>centroid</em> and meant to represent that cluster of data. The <strong>k-means</strong> algorithm is considered a greedy algorithm as at the end every single point in the dataset is part of a specific cluster. Also, <strong>k-means</strong> is an iterative algorithm , given that the process of finding the local minimum is done over multiple iterations. The algorithmic details of <strong>k-means</strong> are provided below. </p><h2 id="How?"><a class="docs-heading-anchor" href="#How?">How?</a><a id="How?-1"></a><a class="docs-heading-anchor-permalink" href="#How?" title="Permalink"></a></h2><p>In <strong>k-means</strong> the algorithm initialized with a set of randomly selected centroids. The number of these centroids is user defined while their location is randomly selected. Based on the <a href="https://en.wikipedia.org/wiki/Euclidean_distance">Euclidean distances</a> of each point in the dataset and the centroids, the points are assigned to each cluster. Here <em>l</em> defines the number of variables, <em>m</em> is the number of points, <em>c_{n}</em> is the number of the clusters, and <em>x</em> is the coordinates on each point/centroid. It should be noted that <em>m</em>, <em>n</em>, and <em>c</em> all must be real numbers and larger than 0. Additionally, the condition of <em>m</em> &gt; <em>n</em> must be fulfilled. This means that the number of points must be larger than the number of clusters.    </p><p class="math-container">\[
d_{d,c_{n}} = \sqrt{\sum _{i=1}^{l} (x_{m} - x_{c_n})^2}
\]</p><p>After assigning the data points to the first set of temporary clusters, the <strong>k-means</strong> algorithm adjusts the centroids by putting them in the center of clusters, only considering the actual data. At this stage the process of distance calculation is repeated again and the points may be reassigned to another cluster based on their distances. This process is repeated until either the location of centroids remain constant or there is no reassignment of the points. These are typical stopping signals for the <strong>k-means</strong> algorithm. </p><h2 id="Practical-Example"><a class="docs-heading-anchor" href="#Practical-Example">Practical Example</a><a id="Practical-Example-1"></a><a class="docs-heading-anchor-permalink" href="#Practical-Example" title="Permalink"></a></h2><p>Let&#39;s build a simple <strong>k-means</strong> algorithm together using some random data: </p><pre><code class="language-julia hljs">using AdvChemStat

c1 = 5 .* rand(5)
cx1 = 1 .+ (2-1) .* rand(5)
c2 = 20 .* rand(5)
cx2 = 6 .+ (10-6) .* rand(5)

data_s = vcat(hcat(cx1,c1),hcat(cx2,c2))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">10×2 Matrix{Float64}:
 1.78769   2.88742
 1.1212    1.44672
 1.09233   1.62924
 1.79091   3.89725
 1.84523   2.10909
 9.66352   9.29768
 7.37362  18.7707
 9.56407   1.37805
 7.42637  13.7752
 7.05905  19.3039</code></pre><pre><code class="language-julia hljs">scatter(data_s[:,1],data_s[:,2],label=false)
xlabel!(&quot;X&quot;)
ylabel!(&quot;Y&quot;)</code></pre><img src="b5858358.svg" alt="Example block output"/><h3 id="Step-1:-Selection-of-centroids"><a class="docs-heading-anchor" href="#Step-1:-Selection-of-centroids">Step 1: Selection of centroids</a><a id="Step-1:-Selection-of-centroids-1"></a><a class="docs-heading-anchor-permalink" href="#Step-1:-Selection-of-centroids" title="Permalink"></a></h3><p>For this case we have only two variables in our dataset <em>data_s</em>. For simplicity, we will start with two clusters, thus <em>k</em> is equal 2. In this case, we can either select two random points within the measurements&#39; window (i.e. between minimum and maximum of values in <em>data_s</em>) or we can choose the indices of two of the measurements at random. Here we go with the second option.  </p><pre><code class="language-julia hljs">k = 2 # k is the number of clusters

ind_c = Int.(round.(1 .+ (size(data_s,1) - 1) .* rand(k))) # index of the centroids</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2-element Vector{Int64}:
 7
 7</code></pre><pre><code class="language-julia hljs">scatter(data_s[:,1],data_s[:,2],label=&quot;data_s&quot;,legend=:topleft)
scatter!(data_s[ind_c,1],data_s[ind_c,2],label=&quot;Centroids&quot;)
xlabel!(&quot;X&quot;)
ylabel!(&quot;Y&quot;)</code></pre><img src="25c46a37.svg" alt="Example block output"/><h3 id="Step-2:-Calculation-of-distances"><a class="docs-heading-anchor" href="#Step-2:-Calculation-of-distances">Step 2: Calculation of distances</a><a id="Step-2:-Calculation-of-distances-1"></a><a class="docs-heading-anchor-permalink" href="#Step-2:-Calculation-of-distances" title="Permalink"></a></h3><p>To calculate the distances, we create a vector containing the distance of each centroid from every single point in the <em>data_s</em>. This will result in a distance matrix of <span>$d_{10 \times 2}$</span>, where column one belongs to the first centroid and the first point corresponds to the first data point in <em>data_s</em>. Since we are using the actual measurements as our starting point, we will have zero distances for at least one point per column.</p><pre><code class="language-julia hljs">d = zeros(size(data_s,1),length(ind_c)) # Generating the distance matrix

for i = 1:length(ind_c)
    cent = transpose(data_s[ind_c[i],:])
    d[:,i] = sqrt.(sum((cent .- data_s).^2,dims=2))

end

d</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">10×2 Matrix{Float64}:
 16.8369    16.8369
 18.4177    18.4177
 18.256     18.256
 15.8866    15.8866
 17.5548    17.5548
  9.74583    9.74583
  0.0        0.0
 17.53      17.53
  4.99578    4.99578
  0.619124   0.619124</code></pre><h3 id="Step-3-Assigning-the-points-to-each-cluster"><a class="docs-heading-anchor" href="#Step-3-Assigning-the-points-to-each-cluster">Step 3 Assigning the points to each cluster</a><a id="Step-3-Assigning-the-points-to-each-cluster-1"></a><a class="docs-heading-anchor-permalink" href="#Step-3-Assigning-the-points-to-each-cluster" title="Permalink"></a></h3><p>Now that we have our distance matrix <em>d</em>, we need to assign each point to a cluster, in our case the two clusters. To do so, we can look at our <em>d</em> matrix row wise and based on the column with the minimum distance assign that point to a cluster. For example, if in <em>d[1,:]</em> the minimum value is located at <em>d[1,2]</em>, then this point belongs to the cluster number two. These calculations can be done using the following code. </p><pre><code class="language-julia hljs">clusters = zeros(size(d))

for i = 1:size(d,1)
    clusters[i,argmin(d[i,:])] = argmin(d[i,:])
end


clusters</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">10×2 Matrix{Float64}:
 1.0  0.0
 1.0  0.0
 1.0  0.0
 1.0  0.0
 1.0  0.0
 1.0  0.0
 1.0  0.0
 1.0  0.0
 1.0  0.0
 1.0  0.0</code></pre><pre><code class="language-julia hljs">scatter(data_s[clusters[:,1] .&gt;0,1],data_s[clusters[:,1] .&gt;0,2],label=&quot;Cluster 1&quot;,legend=:topleft)
scatter!(data_s[clusters[:,2] .&gt;0,1],data_s[clusters[:,2] .&gt;0,2],label=&quot;Cluster 2&quot;)
scatter!(data_s[ind_c,1],data_s[ind_c,2],label=&quot;Centroids&quot;)
xlabel!(&quot;X&quot;)
ylabel!(&quot;Y&quot;)</code></pre><img src="5cc6dc8f.svg" alt="Example block output"/><h3 id="Step-4-Adjusting-the-centroids"><a class="docs-heading-anchor" href="#Step-4-Adjusting-the-centroids">Step 4 Adjusting the centroids</a><a id="Step-4-Adjusting-the-centroids-1"></a><a class="docs-heading-anchor-permalink" href="#Step-4-Adjusting-the-centroids" title="Permalink"></a></h3><p>Now that we have the first set of temporary clusters, we need to recalculate the centroids using the the data points that are assigned to a specific cluster. To do that we calculate the mean of <em>data_s</em> column wise where the values in the <em>clusters</em> matrix is different from zero. </p><pre><code class="language-julia hljs">cents = zeros(size(clusters,2),size(data_s,2))

for i=1:size(clusters,2)

    tv = clusters[:,i]
    #println(tv)
    cents[i,:] = mean(data_s[tv .&gt;0,:],dims=1)

end

cents</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2×2 Matrix{Float64}:
   4.8724    7.44952
 NaN       NaN</code></pre><pre><code class="language-julia hljs">scatter(data_s[clusters[:,1] .&gt;0,1],data_s[clusters[:,1] .&gt;0,2],label=&quot;Cluster 1&quot;,legend=:topleft)
scatter!(data_s[clusters[:,2] .&gt;0,1],data_s[clusters[:,2] .&gt;0,2],label=&quot;Cluster 2&quot;)
scatter!(data_s[ind_c,1],data_s[ind_c,2],label=&quot;Centroids&quot;)
scatter!(cents[:,1],cents[:,2],label=&quot;New centroids&quot;)
xlabel!(&quot;X&quot;)
ylabel!(&quot;Y&quot;)</code></pre><img src="b6459f17.svg" alt="Example block output"/><p>As you can see, the locations of the &quot;New centroids&quot; have changed. Depending on the starting points, the new locations may or may not be closer to the global minimum of the system. The reach such a point, we need to repeat this process multiple times until we do not see any changes in the location of the centroids and thus the reassignment of the points to different clusters. </p><h2 id="Applications"><a class="docs-heading-anchor" href="#Applications">Applications</a><a id="Applications-1"></a><a class="docs-heading-anchor-permalink" href="#Applications" title="Permalink"></a></h2><p>The <strong>k-means</strong> algorithm is mainly used for the clustering of multivariate data. Typically, it follows an HCA or PCA to identify the number of clusters. Then that number is fed to the <strong>k-means</strong> to generate the clusters. When dealing with very large number of variables, <strong>k-means</strong> is not the best algorithm as it may converge to a local minimum rather than a global minimum. It is recommended to run the <strong>k-means</strong> algorithms multiple times to make sure about the robustness of the location of the centroids. In fact most of existing packages for <strong>k-means</strong> have this feature already built in them. </p><h3 id="Packages"><a class="docs-heading-anchor" href="#Packages">Packages</a><a id="Packages-1"></a><a class="docs-heading-anchor-permalink" href="#Packages" title="Permalink"></a></h3><p>There are different implementations of <strong>k-means</strong> algorithm available through <strong>AdvChemStat.jl</strong> package. Below the two main implementations are discussed. </p><h4 id="[Clustering.jl](https://juliastats.org/Clustering.jl/stable/kmeans.html#)"><a class="docs-heading-anchor" href="#[Clustering.jl](https://juliastats.org/Clustering.jl/stable/kmeans.html#)"><a href="https://juliastats.org/Clustering.jl/stable/kmeans.html#">Clustering.jl</a></a><a id="[Clustering.jl](https://juliastats.org/Clustering.jl/stable/kmeans.html#)-1"></a><a class="docs-heading-anchor-permalink" href="#[Clustering.jl](https://juliastats.org/Clustering.jl/stable/kmeans.html#)" title="Permalink"></a></h4><p>The function <em>kmeans(-)</em> is provided via <strong>AdvChemStat.jl</strong>. Please note that the matrix fed to the <em>kmeans(-)</em> should have the variables in rows and measurements in columns. This means that for making it work with our usual datasets, you need to transpose your matrix prior to the model building.   </p><pre><code class="language-julia hljs">k_mod = kmeans(transpose(data_s),2) # to build a k-means model</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Clustering.KmeansResult{Matrix{Float64}, Float64, Int64}([7.880639853489372 2.866904695571122; 15.286857151324872 2.2246282165216584], [2, 2, 2, 2, 2, 1, 1, 2, 1, 1], [1.6039958080210397, 3.652647273813205, 3.5036143257898527, 3.955424659152449, 1.05716108881823, 39.04894944588415, 12.393998522753463, 45.568731128709324, 2.4915696069361957, 16.81180651537636], [4, 6], [4, 6], 130.08789837525427, 3, true)</code></pre><p>Once, the model is built, you are able to get the points assigned to each cluster as well as the coordinates of the centroids. It should be noted that the coordinates of the centroids, similarly to our data are transposed. Thus each represents the coordinates for each centroid rather than the columns. </p><pre><code class="language-julia hljs">a_m = assignments(k_mod) # to get the assignment of the points in the data based on the model
c_c = k_mod.centers</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2×2 Matrix{Float64}:
  7.88064  2.8669
 15.2869   2.22463</code></pre><p>We can also plot our results for visualization.</p><pre><code class="language-julia hljs">scatter(data_s[:,1],data_s[:,2],group=k_mod.assignments,legend=:topleft)
scatter!(c_c[1,:],c_c[2,:],label=&quot;Centroids&quot;)
xlabel!(&quot;X&quot;)
ylabel!(&quot;Y&quot;)</code></pre><img src="ded87629.svg" alt="Example block output"/><p>It should be noted that the julia implementation of <strong>k-means</strong> algorithm does not have an <em>apply(-)</em> incorporated in it meaning that for new data the distances from centroids must be calculated manually for the cluster assignment. This is in-line with the unsupervised nature of the <strong>k-means</strong> algorithm that should not be used for inferences. </p><h4 id="[sklearn.cluster.k_means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)"><a class="docs-heading-anchor" href="#[sklearn.cluster.k_means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans">sklearn.cluster.k_means</a></a><a id="[sklearn.cluster.k_means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)-1"></a><a class="docs-heading-anchor-permalink" href="#[sklearn.cluster.k_means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)" title="Permalink"></a></h4><p>Through <strong>AdvChemStat.jl</strong> package you can also use the python implementation of <strong>k-means</strong> via <strong>scikit-learn</strong>. In this case there is no need for transposing your data. Thus you can use your data as it is (i.e. columns for variables and rows for the measurements). The algorithm outputs a python object containing the coordinates of the centroids and the assigned cluster to each point. Here is an example for our <em>data_s</em>.</p><pre><code class="language-julia hljs">using AdvChemStat

@sk_import cluster: KMeans

kmeans_ = KMeans(n_clusters=2, random_state=0).fit(data_s)


kmeans_.cluster_centers_</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2×2 Matrix{Float64}:
 7.88064  15.2869
 2.8669    2.22463</code></pre><p>We can also plot the results as shown below. Please note that in case of python implementation you can use the function <em>apply(-)</em> to perform prediction using the built model.</p><pre><code class="language-julia hljs">scatter(data_s[:,1],data_s[:,2],group=kmeans_.labels_,legend=:topleft)
scatter!(kmeans_.cluster_centers_[:,1],kmeans_.cluster_centers_[:,2],label=&quot;Centroids&quot;)
xlabel!(&quot;X&quot;)
ylabel!(&quot;Y&quot;)
</code></pre><h2 id="Additional-Example"><a class="docs-heading-anchor" href="#Additional-Example">Additional Example</a><a id="Additional-Example-1"></a><a class="docs-heading-anchor-permalink" href="#Additional-Example" title="Permalink"></a></h2><p>If you are interested in practicing more, you can use the <a href="https://github.com/EMCMS/AdvChemStat.jl/blob/main/datasets/NIR.csv">NIR.csv</a> file provided in the <a href="https://github.com/EMCMS/AdvChemStat.jl/tree/main/datasets">folder dataset</a> of the package <a href="https://github.com/EMCMS/AdvChemStat.jl"><em>AdvChemStat.jl</em> github repository</a>. You can try to use the NIR spectra and <strong>k-means</strong> to see whether there are clear clusters of samples associated with different octane number. </p><p>If you are interested in additional information about <strong>k-means</strong> and would like to know more you can check this <a href="https://ocw.mit.edu/courses/6-0002-introduction-to-computational-thinking-and-data-science-fall-2016/resources/lecture-12-clustering/">MIT course material</a>.  </p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../HCA/">« HCA</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.2.1 on <span class="colophon-date" title="Friday 26 January 2024 19:33">Friday 26 January 2024</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
